{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059bfb0e-23a8-44a6-816c-d10baf773627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet Topic Modeling with BERTopic\n",
    "# ============================\n",
    "\n",
    "# This notebook performs topic modeling on a large database of tweets using BERTopic.\n",
    "# Since the database is large, we use efficient loading techniques and batch processing.\n",
    "# We use pysentimiento for specialized tweet preprocessing before applying BERTopic.\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f20a72-6c52-468c-908a-45a45eaf7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection function\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"Connect to the SQLite database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Successfully connected to database: {db_path}\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "# 1. Initial Database Exploration\n",
    "# ===============================\n",
    "\n",
    "# Connect to the database\n",
    "db_path = \"english_tweets_only.db\"\n",
    "conn = connect_to_db(db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9cfb475-0f15-4042-811f-f48c0cb4be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get table info\n",
    "def get_table_info(conn, table_name):\n",
    "    \"\"\"Get information about the specified table.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    print(\"=\" * (len(table_name) + 7))\n",
    "    for col in columns:\n",
    "        print(f\"{col[1]} ({col[2]})\")\n",
    "\n",
    "# Get table information\n",
    "get_table_info(conn, \"posts\")\n",
    "get_table_info(conn, \"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9616373c-dfae-4138-b392-1383c2010abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get table statistics\n",
    "def get_table_stats(conn, table_name):\n",
    "    \"\"\"Get basic statistics about the table.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"\\nTable '{table_name}' contains {count:,} records\")\n",
    "    \n",
    "    if table_name == \"posts\":\n",
    "        # Get date range\n",
    "        cursor.execute(\"SELECT MIN(date), MAX(date) FROM posts\")\n",
    "        min_date, max_date = cursor.fetchone()\n",
    "        print(f\"Date range: {min_date} to {max_date}\")\n",
    "        \n",
    "        # Get top 5 authors by post count\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT author, COUNT(*) as post_count \n",
    "            FROM posts \n",
    "            GROUP BY author \n",
    "            ORDER BY post_count DESC \n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        print(\"\\nTop 5 authors by post count:\")\n",
    "        for author, count in cursor.fetchall():\n",
    "            print(f\"  {author}: {count:,} posts\")\n",
    "\n",
    "# Get table statistics\n",
    "get_table_stats(conn, \"posts\")\n",
    "get_table_stats(conn, \"authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d821884-80a3-4b19-930f-9e825d0c8367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 14:35:19,327 - __main__ - INFO - Successfully connected to database at 53k_individual_hcps_70_percent_confidence_tweets_2019_2022.db\n",
      "2025-04-14 14:35:21,996 - __main__ - WARNING - No tweets found with language='en'. Fetching all tweets to analyze language...\n",
      "2025-04-14 14:38:14,363 - __main__ - INFO - Fetched 16616970 total tweets for language detection\n",
      "2025-04-14 14:38:21,827 - __main__ - INFO - Detecting English tweets from mixed language dataset...\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Preprocessing\n",
    "# ====================\n",
    "\n",
    "def preprocess_tweets(text):\n",
    "    \"\"\"Clean and preprocess tweet text using pysentimiento and additional cleaning.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Use pysentimiento's specialized tweet preprocessor first\n",
    "    # This handles mentions, URLs, emojis, and other Twitter-specific elements\n",
    "    processed_text = preprocess_tweet(text)\n",
    "    \n",
    "    # Additional cleaning\n",
    "    # Remove any remaining special characters and numbers\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', processed_text)\n",
    "    processed_text = re.sub(r'\\d+', '', processed_text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    processed_text = processed_text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0d694-706e-4633-9fc0-32b0edd00563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Efficient Data Loading\n",
    "# ========================\n",
    "\n",
    "def load_tweets_in_batches(conn, batch_size=10000, max_tweets=None):\n",
    "    \"\"\"\n",
    "    Load tweets in batches to handle large datasets efficiently.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        batch_size: Number of tweets to load in each batch\n",
    "        max_tweets: Maximum number of tweets to load (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        List of preprocessed tweet texts\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get total count if needed\n",
    "    if max_tweets is None:\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM posts\")\n",
    "        max_tweets = cursor.fetchone()[0]\n",
    "    \n",
    "    # Initialize variables\n",
    "    all_tweets = []\n",
    "    offset = 0\n",
    "    total_loaded = 0\n",
    "    \n",
    "    print(f\"Loading up to {max_tweets:,} tweets in batches of {batch_size:,}\")\n",
    "    \n",
    "    # Load in batches\n",
    "    pbar = tqdm(total=min(max_tweets, max_tweets))\n",
    "    while total_loaded < max_tweets:\n",
    "        # Adjust batch size for last batch if needed\n",
    "        current_batch_size = min(batch_size, max_tweets - total_loaded)\n",
    "        \n",
    "        # Execute query for current batch\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT content \n",
    "            FROM posts \n",
    "            LIMIT {current_batch_size} OFFSET {offset}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Process batch\n",
    "        batch_tweets = [preprocess_tweets(row[0]) for row in cursor.fetchall()]\n",
    "        batch_tweets = [tweet for tweet in batch_tweets if len(tweet) > 20]  # Filter very short tweets\n",
    "        \n",
    "        # Add to collection\n",
    "        all_tweets.extend(batch_tweets)\n",
    "        \n",
    "        # Update counters\n",
    "        batch_actual_size = len(batch_tweets)\n",
    "        total_loaded += batch_actual_size\n",
    "        offset += current_batch_size\n",
    "        pbar.update(batch_actual_size)\n",
    "        \n",
    "        # Break if no more tweets\n",
    "        if batch_actual_size == 0:\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Loaded {len(all_tweets):,} tweets after preprocessing and filtering\")\n",
    "    return all_tweets\n",
    "\n",
    "# Load tweets (adjust max_tweets as needed for initial testing)\n",
    "# For initial testing, you might want to use a smaller sample\n",
    "tweets = load_tweets_in_batches(conn, batch_size=10000, max_tweets=100000)  # Adjust max_tweets as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567563fb-c60d-43dc-9bbc-d80bfeaba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Topic Modeling with BERTopic\n",
    "# ==============================\n",
    "\n",
    "def create_topic_model(tweets, nr_topics=\"auto\"):\n",
    "    \"\"\"\n",
    "    Create and train a BERTopic model.\n",
    "    \n",
    "    Args:\n",
    "        tweets: List of preprocessed tweet texts\n",
    "        nr_topics: Number of topics to extract (\"auto\" to determine automatically)\n",
    "        \n",
    "    Returns:\n",
    "        Trained BERTopic model\n",
    "    \"\"\"\n",
    "    print(\"Setting up the topic modeling pipeline...\")\n",
    "    \n",
    "    # Create sentence transformer model for embeddings\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Smaller model for efficiency\n",
    "    \n",
    "    # Set up dimensionality reduction\n",
    "    umap_model = UMAP(n_neighbors=15, \n",
    "                      n_components=5,\n",
    "                      min_dist=0.0, \n",
    "                      metric='cosine', \n",
    "                      random_state=42)\n",
    "    \n",
    "    # Set up clustering model\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10,\n",
    "                                   min_samples=5,\n",
    "                                   metric='euclidean',\n",
    "                                   cluster_selection_method='eom',\n",
    "                                   prediction_data=True)\n",
    "    \n",
    "    # Set up custom vectorizer with additional stopwords\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    additional_stopwords = ['rt', 'RT', 'amp', 'twitter', 'tweet', 'tweeting']\n",
    "    stop_words.extend(additional_stopwords)\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(stop_words=stop_words)\n",
    "    \n",
    "    # Create the BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        nr_topics=nr_topics,\n",
    "        calculate_probabilities=False,  # Turn off for large datasets to save memory\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"Training BERTopic model...\")\n",
    "    topics, probs = topic_model.fit_transform(tweets)\n",
    "    \n",
    "    print(f\"Model training complete. Found {len(topic_model.get_topic_info())-1} topics.\")\n",
    "    return topic_model, topics\n",
    "\n",
    "# Create and train the model\n",
    "# This may take time depending on your dataset size and available compute resources\n",
    "topic_model, topics = create_topic_model(tweets, nr_topics=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d6293-a610-4902-a5c0-830cfcf30dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyzing Topics\n",
    "# =================\n",
    "\n",
    "# Get topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"\\nTopic Distribution:\")\n",
    "print(topic_info.head(10))\n",
    "\n",
    "# Visualize topic size distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sizes = topic_info[topic_info['Topic'] != -1]['Count'].values\n",
    "plt.hist(sizes, bins=30)\n",
    "plt.xlabel('Topic Size (Number of Tweets)')\n",
    "plt.ylabel('Number of Topics')\n",
    "plt.title('Distribution of Topic Sizes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get the top topics (excluding the -1 outlier topic)\n",
    "top_topics = topic_info[topic_info['Topic'] != -1].head(10)['Topic'].values\n",
    "\n",
    "# Print the top terms for top topics\n",
    "print(\"\\nTop Terms for Major Topics:\")\n",
    "for topic in top_topics:\n",
    "    print(f\"\\nTopic {topic}:\")\n",
    "    for term, weight in topic_model.get_topic(topic)[:10]:\n",
    "        print(f\"  {term}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1f35e-0d92-4f0e-9ddf-0035ea721b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualizations\n",
    "# ================\n",
    "\n",
    "# Topic similarity map\n",
    "print(\"\\nGenerating topic similarity visualization...\")\n",
    "try:\n",
    "    fig = topic_model.visualize_topics()\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate topic similarity visualization: {e}\")\n",
    "\n",
    "# Topic word scores\n",
    "print(\"\\nGenerating word score visualizations for top topics...\")\n",
    "for topic in top_topics[:5]:  # Show for first 5 top topics\n",
    "    try:\n",
    "        fig = topic_model.visualize_barchart(topics=[topic], top_n_topics=1)\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate word score visualization for topic {topic}: {e}\")\n",
    "\n",
    "# Topic hierarchy\n",
    "print(\"\\nGenerating topic hierarchy visualization...\")\n",
    "try:\n",
    "    fig = topic_model.visualize_hierarchy()\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate topic hierarchy visualization: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a56339-2373-486e-8c81-555431ab2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Dynamic Topic Modeling (Optional)\n",
    "# ==================================\n",
    "\n",
    "def perform_dynamic_topic_modeling(conn, topic_model, time_periods=5):\n",
    "    \"\"\"\n",
    "    Analyze how topics evolve over time.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        topic_model: Trained BERTopic model\n",
    "        time_periods: Number of time periods to divide the data into\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming dynamic topic modeling...\")\n",
    "    \n",
    "    # Get date range\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT MIN(date), MAX(date) FROM posts\")\n",
    "    min_date, max_date = cursor.fetchone()\n",
    "    \n",
    "    try:\n",
    "        # Convert to datetime if possible\n",
    "        min_date = pd.to_datetime(min_date)\n",
    "        max_date = pd.to_datetime(max_date)\n",
    "        \n",
    "        # Create time bins\n",
    "        date_bins = pd.date_range(start=min_date, end=max_date, periods=time_periods+1)\n",
    "        date_labels = [f\"Period {i+1}\" for i in range(time_periods)]\n",
    "        \n",
    "        # Fetch tweets with dates\n",
    "        cursor.execute(\"SELECT content, date FROM posts LIMIT 50000\")  # Limit for memory considerations\n",
    "        df = pd.DataFrame(cursor.fetchall(), columns=['content', 'date'])\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df = df.dropna(subset=['date'])\n",
    "        \n",
    "        # Assign time periods\n",
    "        df['time_period'] = pd.cut(df['date'], bins=date_bins, labels=date_labels, include_lowest=True)\n",
    "        \n",
    "        # Preprocess tweets\n",
    "        df['processed_content'] = df['content'].apply(preprocess_tweets)\n",
    "        \n",
    "        # Filter out short tweets\n",
    "        df = df[df['processed_content'].str.len() > 20]\n",
    "        \n",
    "        # Get documents and timestamps\n",
    "        documents = df['processed_content'].tolist()\n",
    "        timestamps = df['time_period'].tolist()\n",
    "        \n",
    "        # Run dynamic topic modeling\n",
    "        topics_over_time = topic_model.topics_over_time(documents, timestamps)\n",
    "        \n",
    "        # Visualize\n",
    "        fig = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=10)\n",
    "        fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in dynamic topic modeling: {e}\")\n",
    "        print(\"Skipping dynamic topic analysis.\")\n",
    "\n",
    "# Optional: Uncomment to run dynamic topic modeling\n",
    "# perform_dynamic_topic_modeling(conn, topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045d499-8243-4b46-b5d3-42ea123307d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Export Results\n",
    "# ===============\n",
    "\n",
    "# Save model\n",
    "topic_model.save(\"tweet_bertopic_model\")\n",
    "\n",
    "# Export topic information to CSV\n",
    "topic_info.to_csv(\"tweet_topics_info.csv\", index=False)\n",
    "\n",
    "# Export top terms for all topics\n",
    "all_topics_terms = {}\n",
    "for topic in topic_info['Topic'].values:\n",
    "    if topic != -1:  # Skip outlier topic\n",
    "        all_topics_terms[topic] = topic_model.get_topic(topic)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "topics_df = pd.DataFrame({\n",
    "    'Topic': [topic for topic in all_topics_terms.keys() for _ in range(10)],\n",
    "    'Term': [term for terms in all_topics_terms.values() for term, _ in terms[:10]],\n",
    "    'Weight': [weight for terms in all_topics_terms.values() for _, weight in terms[:10]]\n",
    "})\n",
    "topics_df.to_csv(\"tweet_topic_terms.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f054ac5-a1ba-4426-830e-33206d14ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Assign Topics to All Tweets\n",
    "# ============================\n",
    "\n",
    "def assign_topics_to_tweets(conn, topic_model, batch_size=10000, max_tweets=None):\n",
    "    \"\"\"\n",
    "    Assign topics to all tweets in the database.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        topic_model: Trained BERTopic model\n",
    "        batch_size: Number of tweets to process in each batch\n",
    "        max_tweets: Maximum number of tweets to process (None for all)\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get total count if needed\n",
    "    if max_tweets is None:\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM posts\")\n",
    "        max_tweets = cursor.fetchone()[0]\n",
    "    \n",
    "    # Initialize variables\n",
    "    offset = 0\n",
    "    total_processed = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Assigning topics to up to {max_tweets:,} tweets in batches of {batch_size:,}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    pbar = tqdm(total=min(max_tweets, max_tweets))\n",
    "    while total_processed < max_tweets:\n",
    "        # Adjust batch size for last batch if needed\n",
    "        current_batch_size = min(batch_size, max_tweets - total_processed)\n",
    "        \n",
    "        # Execute query for current batch\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT post_id, content \n",
    "            FROM posts \n",
    "            LIMIT {current_batch_size} OFFSET {offset}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_data = cursor.fetchall()\n",
    "        if not batch_data:\n",
    "            break\n",
    "            \n",
    "        batch_ids = [row[0] for row in batch_data]\n",
    "        batch_tweets = [preprocess_tweets(row[1]) for row in batch_data]\n",
    "        \n",
    "        # Predict topics\n",
    "        try:\n",
    "            batch_topics, _ = topic_model.transform(batch_tweets)\n",
    "            \n",
    "            # Collect results\n",
    "            for tweet_id, topic in zip(batch_ids, batch_topics):\n",
    "                results.append((tweet_id, topic))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch at offset {offset}: {e}\")\n",
    "            \n",
    "        # Update counters\n",
    "        batch_actual_size = len(batch_data)\n",
    "        total_processed += batch_actual_size\n",
    "        offset += current_batch_size\n",
    "        pbar.update(batch_actual_size)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"Assigned topics to {len(results):,} tweets\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    topics_df = pd.DataFrame(results, columns=['post_id', 'topic'])\n",
    "    topics_df.to_csv(\"tweet_topic_assignments.csv\", index=False)\n",
    "    \n",
    "    return topics_df\n",
    "\n",
    "# Optional: Uncomment to assign topics to all tweets\n",
    "# tweet_topics = assign_topics_to_tweets(conn, topic_model, batch_size=10000, max_tweets=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d95efb-238f-46eb-8c02-ab59f01f1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Topic Analysis by Author Metrics\n",
    "# =================================\n",
    "\n",
    "def analyze_topics_by_author_metrics(conn, tweet_topics):\n",
    "    \"\"\"\n",
    "    Analyze how topics correlate with author metrics.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        tweet_topics: DataFrame with post_id and topic\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing topics by author metrics...\")\n",
    "    \n",
    "    try:\n",
    "        # Join posts with authors and topics\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            t.topic,\n",
    "            a.followers_count,\n",
    "            a.statuses_count,\n",
    "            p.retweet_count,\n",
    "            p.like_count,\n",
    "            p.reply_count\n",
    "        FROM \n",
    "            posts p\n",
    "        JOIN \n",
    "            authors a ON p.author_osn_id = a.author_osn_id\n",
    "        JOIN \n",
    "            tweet_topic_assignments t ON p.post_id = t.post_id\n",
    "        WHERE\n",
    "            t.topic != -1\n",
    "        LIMIT 100000\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create temporary table for topic assignments\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS tweet_topic_assignments\")\n",
    "        cursor.execute(\"CREATE TEMPORARY TABLE tweet_topic_assignments (post_id REAL, topic INTEGER)\")\n",
    "        \n",
    "        # Insert topic assignments in batches\n",
    "        chunk_size = 10000\n",
    "        for i in range(0, len(tweet_topics), chunk_size):\n",
    "            chunk = tweet_topics.iloc[i:i+chunk_size]\n",
    "            chunk_data = [(row.post_id, row.topic) for _, row in chunk.iterrows()]\n",
    "            cursor.executemany(\"INSERT INTO tweet_topic_assignments VALUES (?, ?)\", chunk_data)\n",
    "        \n",
    "        # Run the query\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Calculate metrics by topic\n",
    "        topic_metrics = df.groupby('topic').agg({\n",
    "            'followers_count': 'mean',\n",
    "            'statuses_count': 'mean',\n",
    "            'retweet_count': 'mean',\n",
    "            'like_count': 'mean',\n",
    "            'reply_count': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Save results\n",
    "        topic_metrics.to_csv(\"topic_author_metrics.csv\", index=False)\n",
    "        \n",
    "        # Visualize relationship between topics and engagement\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Sort topics by engagement (retweet + like + reply counts)\n",
    "        topic_metrics['total_engagement'] = topic_metrics['retweet_count'] + topic_metrics['like_count'] + topic_metrics['reply_count']\n",
    "        topic_metrics = topic_metrics.sort_values('total_engagement', ascending=False)\n",
    "        \n",
    "        # Plot top 20 topics by engagement\n",
    "        top_n = min(20, len(topic_metrics))\n",
    "        topics = topic_metrics['topic'].values[:top_n]\n",
    "        \n",
    "        plt.bar(range(top_n), topic_metrics['total_engagement'].values[:top_n])\n",
    "        plt.xticks(range(top_n), topics, rotation=90)\n",
    "        plt.xlabel('Topic')\n",
    "        plt.ylabel('Average Engagement (RT + Like + Reply)')\n",
    "        plt.title('Topics by Average Engagement')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in topic-author analysis: {e}\")\n",
    "\n",
    "# Optional: Uncomment to analyze topics by author metrics\n",
    "# analyze_topics_by_author_metrics(conn, tweet_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e060c4-1713-4fc1-9709-83da4013f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Close Connection\n",
    "# ==================\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n",
    "print(\"\\nAnalysis complete! Results saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_project",
   "language": "python",
   "name": "env_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
