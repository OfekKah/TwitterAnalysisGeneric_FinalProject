{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e823dcd-3a31-4ea4-a73b-60f7bb9a3cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shayher/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shayher/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shayher/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling with LDA - Tweet Analysis Notebook\n",
    "# !pip install gensim pyLDAvis wordcloud tqdm pysentimiento nltk pandas numpy matplotlib seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "import logging\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.dates as mdates\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "output_dir = \"lda_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    from pysentimiento.preprocessing import preprocess_tweet\n",
    "except ImportError:\n",
    "    print(\"pysentimiento not installed. Using basic tweet preprocessing.\")\n",
    "    def preprocess_tweet(text):\n",
    "        \"\"\"Basic tweet preprocessing if pysentimiento is not available\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        return text\n",
    "\n",
    "# Try importing gensim and pyLDAvis, provide error messages if not available\n",
    "try:\n",
    "    from gensim import corpora, models\n",
    "    from gensim.models import CoherenceModel\n",
    "except ImportError:\n",
    "    print(\"ERROR: gensim is not installed. Please install it with 'pip install gensim'\")\n",
    "    print(\"Topic modeling functionality will not work without gensim.\")\n",
    "\n",
    "try:\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim_models\n",
    "except ImportError:\n",
    "    print(\"WARNING: pyLDAvis is not installed. Visualization will not be available.\")\n",
    "    print(\"Install it with 'pip install pyLDAvis'\")\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    print(\"If you encounter NLTK resource errors, run these commands in a separate cell:\")\n",
    "    print(\"import nltk\")\n",
    "    print(\"nltk.download('punkt')\")\n",
    "    print(\"nltk.download('stopwords')\")\n",
    "    print(\"nltk.download('wordnet')\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.getLogger('gensim').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337aacf8-4625-4467-8d9e-58f5f9979705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux dt-3090-01 5.14.0-503.15.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Nov 26 17:24:29 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0620fad7-8bcc-4a94-9b65-46654f5f46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_path=\"LDA_config.yaml\"):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51bc336-75a6-4e40-a8e9-ce2e2bba1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database and load data\n",
    "def load_data(db_path, table_name='posts', content_column='content', timestamp_column='date', limit=None):\n",
    "    \"\"\"\n",
    "    Load data from SQLite database\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    db_path : str\n",
    "        Path to the SQLite database\n",
    "    table_name : str\n",
    "        Name of the table to query\n",
    "    content_column : str\n",
    "        Name of the column containing text data\n",
    "    limit : int, optional\n",
    "        Limit the number of rows to fetch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the data\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    if limit:\n",
    "        query = f\"SELECT * FROM {table_name} LIMIT {limit}\"\n",
    "    else:\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Check if the content column exists\n",
    "    if content_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{content_column}' not found in the data\")\n",
    "    \n",
    "    # Remove rows with empty content\n",
    "    df = df[df[content_column].notna()]\n",
    "\n",
    "    if timestamp_column in df.columns:\n",
    "        df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "    \n",
    "    print(f\"Loaded {len(df)} records from {table_name}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff13fe7-8865-49f9-8b70-c956841016f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_text(text, is_tweet=True):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to preprocess\n",
    "    is_tweet : bool\n",
    "        Whether the text is a tweet (uses pysentimiento preprocessing)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    if is_tweet:\n",
    "        text = preprocess_tweet(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbc6a41-bcfd-472e-b6cd-59791567824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return word_tokenize(text)\n",
    "    except LookupError:\n",
    "        \n",
    "        # Simple split by spaces as fallback\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16025cba-136b-4c88-8c82-9c22e9db3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens : list\n",
    "        List of tokens\n",
    "    custom_stopwords : list, optional\n",
    "        List of additional stopwords to remove\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        # Fallback if NLTK stopwords are not available\n",
    "        print(\"NLTK stopwords not available. Using a basic stopword list.\")\n",
    "        stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "                      'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "                      'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "                      'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    "                      'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "                      'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "                      'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "                      'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                      'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "                      'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "                      'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once'\n",
    "                      'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "                        'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', \n",
    "                        'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'don', \n",
    "                        'should', 'now', 'd', 'll', 'm', 'o', 're', 's', 't', 've', 'y', 'ain', \n",
    "                        'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', \n",
    "                        'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n",
    "                        'rt', 'via', 'https', 'http', 'www', 'com', 'co', 'amp', 'u', 'ur',\n",
    "                        'get', 'like', 'got', 'one', 'im', 'yeah', 'oh', 'lol', 'hey', 'ok', 'okay', \n",
    "                        'hi', 'ha', 'haha', 'yes', 'nope', 'thanks', 'thank', 'pls', 'please'}\n",
    "    \n",
    "    if custom_stopwords:\n",
    "        stop_words.update(custom_stopwords)\n",
    "    \n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da54b84-ba20-4c36-a5e7-c93821d2b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize tokens\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens : list\n",
    "        List of tokens\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of lemmatized tokens\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    except LookupError:        \n",
    "        return tokens                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800499f9-1de4-4c47-85ed-db084d75e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(documents, is_tweet=True, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Preprocess a list of documents\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : list\n",
    "        List of text documents\n",
    "    is_tweet : bool\n",
    "        Whether the documents are tweets\n",
    "    custom_stopwords : list, optional\n",
    "        List of additional stopwords to remove\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of preprocessed documents as lists of tokens\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    # Disable tqdm completely to avoid errors\n",
    "    print(f\"Processing {len(documents)} documents...\")\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # Print progress periodically\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print(f\"Processed {i}/{len(documents)} documents\")\n",
    "            \n",
    "        clean_doc = clean_text(doc, is_tweet=is_tweet)\n",
    "        tokens = tokenize(clean_doc)\n",
    "        tokens = remove_stopwords(tokens, custom_stopwords)\n",
    "        tokens = lemmatize(tokens)\n",
    "        \n",
    "        # Only add documents that have tokens left after preprocessing\n",
    "        if tokens and len(tokens) > 3:\n",
    "            processed_docs.append(tokens)\n",
    "    \n",
    "    print(f\"Finished processing. {len(processed_docs)} documents retained after preprocessing.\")\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a204e21b-75db-4e03-b4c5-f16294cbe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "def add_bigrams_to_documents(processed_docs, min_count=5, threshold=100):\n",
    "    \"\"\"\n",
    "    Add bigrams to preprocessed documents using gensim's Phrases.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_docs : list of list of str\n",
    "        Tokenized and cleaned documents\n",
    "    min_count : int\n",
    "        Minimum count of word pairs to be considered as bigram (e.g., 5)\n",
    "    threshold : int\n",
    "        Threshold for forming the phrases (higher = fewer phrases)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of list of str\n",
    "        Documents with bigrams added (e.g., ['donald_trump', 'white_house'])\n",
    "    \"\"\"\n",
    "    # Detect common bigrams across the corpus\n",
    "    bigram_model = Phrases(processed_docs, min_count=min_count, threshold=threshold)\n",
    "    bigram_phraser = Phraser(bigram_model)\n",
    "\n",
    "    # Apply the bigram model to all documents\n",
    "    bigrammed_docs = [bigram_phraser[doc] for doc in processed_docs]\n",
    "    return bigrammed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4289083c-436a-4915-8b19-91922576a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling functions\n",
    "def create_dictionary_and_corpus(processed_docs, no_below=5, no_above=0.5):\n",
    "    \"\"\"\n",
    "    Create a dictionary and corpus from preprocessed documents\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_docs : list\n",
    "        List of preprocessed documents (lists of tokens)\n",
    "    no_below : int\n",
    "        Keep tokens that appear in at least this many documents\n",
    "    no_above : float\n",
    "        Keep tokens that appear in at most this fraction of documents\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (dictionary, corpus)\n",
    "    \"\"\"\n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    \n",
    "    # Filter extremes\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create corpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "    print(f\"Dictionary size: {len(dictionary)}\")\n",
    "    print(f\"Corpus size: {len(corpus)}\")\n",
    "    \n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8a15cd-3762-4b47-adcc-de7df366bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(corpus, dictionary, num_topics=10, passes=25, iterations=50, alpha='auto', eta='auto'):\n",
    "    \"\"\"\n",
    "    Trains a gensim Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "    Args:\n",
    "        corpus (list): Document-term matrix in BoW format.\n",
    "        dictionary (gensim.corpora.Dictionary): Gensim word-to-ID mapping.\n",
    "        num_topics (int): Number of topics to find. Defaults to 10.\n",
    "        passes (int): Number of training passes. Defaults to 15.\n",
    "        iterations (int): Iterations per pass. Defaults to 50.\n",
    "        alpha (str or float): Document-topic prior. Defaults to 'auto'.\n",
    "        eta (str or float): Topic-word prior. Defaults to 'auto'.\n",
    "\n",
    "    Returns:\n",
    "        gensim.models.LdaModel: The trained LDA model object.\n",
    "    \"\"\"\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        random_state=42,\n",
    "        iterations=iterations\n",
    "    )\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba42d87-299f-4d61-aa05-4100eb8f3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_score(model, corpus, dictionary, processed_docs, coherence='c_v'):\n",
    "    \"\"\"\n",
    "    Compute coherence score for an LDA model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : gensim.models.LdaModel\n",
    "        Trained LDA model\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    dictionary : gensim.corpora.Dictionary\n",
    "        Dictionary mapping words to indices\n",
    "    processed_docs : list\n",
    "        List of preprocessed documents (lists of tokens)\n",
    "    coherence : str\n",
    "        Coherence measure to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Coherence score\n",
    "    \"\"\"\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=processed_docs,\n",
    "        dictionary=dictionary,\n",
    "        coherence=coherence\n",
    "    )\n",
    "    \n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5a4ab5-436d-46ee-bc81-fb6f40c1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_models(corpus, dictionary, processed_docs, min_topics=70, max_topics=100, step=5, iterations=50, alpha='auto', eta='auto'):\n",
    "    \"\"\"\n",
    "    Evaluate LDA models with different numbers of topics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    dictionary : gensim.corpora.Dictionary\n",
    "        Dictionary mapping words to indices\n",
    "    processed_docs : list\n",
    "        List of preprocessed documents (lists of tokens)\n",
    "    min_topics : int\n",
    "        Minimum number of topics to evaluate\n",
    "    max_topics : int\n",
    "        Maximum number of topics to evaluate\n",
    "    step : int\n",
    "        Step size for number of topics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (coherence_scores, perplexity_scores)\n",
    "    \"\"\"\n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    \n",
    "    for num_topics in range(min_topics, max_topics + 1, step):\n",
    "        print(f\"Evaluating model with {num_topics} topics...\")\n",
    "        \n",
    "        lda_model = train_lda_model(corpus, dictionary, num_topics=num_topics, iterations=iterations, alpha=alpha, eta=eta)\n",
    "        \n",
    "        # Compute coherence score\n",
    "        coherence_score = compute_coherence_score(lda_model, corpus, dictionary, processed_docs)\n",
    "        coherence_scores.append(coherence_score)\n",
    "        \n",
    "        # Compute perplexity\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "        perplexity_scores.append(perplexity)\n",
    "        \n",
    "        print(f\"Coherence score: {coherence_score:.4f}\")\n",
    "        print(f\"Perplexity: {perplexity:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return coherence_scores, perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e47b42-7c1d-475c-a25c-533e63ab80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_evaluation(topic_range, coherence_scores, perplexity_scores):\n",
    "    \"\"\"\n",
    "    Plot coherence scores and perplexity scores for different numbers of topics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic_range : list\n",
    "        List of numbers of topics\n",
    "    coherence_scores : list\n",
    "        List of coherence scores\n",
    "    perplexity_scores : list\n",
    "        List of perplexity scores\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot coherence scores\n",
    "    ax1.plot(topic_range, coherence_scores, 'o-')\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('Coherence Score')\n",
    "    ax1.set_title('Coherence Score by Number of Topics')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot perplexity scores\n",
    "    ax2.plot(topic_range, perplexity_scores, 'o-')\n",
    "    ax2.set_xlabel('Number of Topics')\n",
    "    ax2.set_ylabel('Perplexity Score')\n",
    "    ax2.set_title('Perplexity Score by Number of Topics')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"topic_evaluation.png\"), bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c35728c-6047-499a-9ebd-5d1e723bcd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_wordclouds(lda_model, num_topics):\n",
    "    \"\"\"\n",
    "    Create word clouds for each topic\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        Trained LDA model\n",
    "    num_topics : int\n",
    "        Number of topics\n",
    "    \"\"\"\n",
    "    # Create subplots\n",
    "    cols = 3\n",
    "    rows = (num_topics + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(18, 4 * rows))\n",
    "    \n",
    "    # Flatten axes array if needed\n",
    "    if rows > 1:\n",
    "        axes = axes.flatten()\n",
    "    elif cols > 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Create word cloud for each topic\n",
    "    for i, (topic_num, ax) in enumerate(zip(range(num_topics), axes)):\n",
    "        topic_words = dict(lda_model.show_topic(topic_num, 30))\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=400,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            max_words=50,\n",
    "            prefer_horizontal=1.0\n",
    "        ).generate_from_frequencies(topic_words)\n",
    "        \n",
    "        # Display word cloud\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.set_title(f'Topic {topic_num + 1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_topics, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    output_path = \"lda_output/topic_wordclouds.png\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d9d699a-8b6e-4bf0-9a35-057749697707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topics_pyldavis(lda_model, corpus, dictionary):\n",
    "    \"\"\"\n",
    "    Visualize topics using pyLDAvis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        Trained LDA model\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    dictionary : gensim.corpora.Dictionary\n",
    "        Dictionary mapping words to indices\n",
    "    \"\"\"\n",
    "    # Prepare visualization\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c01dc06a-2571-460f-a616-11dbf97225e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic_per_document(lda_model, corpus):\n",
    "    \"\"\"\n",
    "    Get the dominant topic for each document\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        Trained LDA model\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with dominant topic information\n",
    "    \"\"\"\n",
    "    # Get topic distribution for each document\n",
    "    topic_distributions = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "    \n",
    "    # Get dominant topics\n",
    "    dominant_topics = []\n",
    "    for i, dist in enumerate(topic_distributions):\n",
    "        # Sort by topic probability\n",
    "        sorted_topics = sorted(dist, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top topic\n",
    "        if sorted_topics:\n",
    "            top_topic_id = sorted_topics[0][0]\n",
    "            top_topic_prob = sorted_topics[0][1]\n",
    "            \n",
    "            # Get top words for this topic\n",
    "            top_words = [word for word, _ in lda_model.show_topic(top_topic_id, 5)]\n",
    "            top_words_str = \", \".join(top_words)\n",
    "            \n",
    "            dominant_topics.append({\n",
    "                'document_id': i,\n",
    "                'dominant_topic': top_topic_id,\n",
    "                'topic_probability': top_topic_prob,\n",
    "                'top_words': top_words_str\n",
    "            })\n",
    "        else:\n",
    "            dominant_topics.append({\n",
    "                'document_id': i,\n",
    "                'dominant_topic': -1,\n",
    "                'topic_probability': 0.0,\n",
    "                'top_words': \"\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(dominant_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4f59935-ba74-484d-b17c-5fd7847cade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_topic_trends(\n",
    "    original_df_with_timestamps,\n",
    "    dominant_topics_info_df,\n",
    "    timestamp_col='date',\n",
    "    topic_assignment_col='dominant_topic',\n",
    "    num_total_topics=None, \n",
    "    time_freq='M',\n",
    "    figure_size=(14, 8),\n",
    "    colormap_name='tab20' \n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and displays a stacked area chart of topic trends over time.\n",
    "\n",
    "    Args:\n",
    "        original_df_with_timestamps (pd.DataFrame): DataFrame containing the original data,\n",
    "                                                    including the timestamp column.\n",
    "        dominant_topics_info_df (pd.DataFrame): DataFrame with topic assignments.\n",
    "                                                Must have an index that aligns with\n",
    "                                                original_df_with_timestamps and a column\n",
    "                                                specified by topic_assignment_col.\n",
    "        timestamp_col (str): Name of the column in original_df_with_timestamps\n",
    "                             that contains the timestamps.\n",
    "        topic_assignment_col (str): Name of the column in dominant_topics_info_df\n",
    "                                    that contains the dominant topic ID.\n",
    "        num_total_topics (int): The total number of topics generated by the LDA model.\n",
    "                                This is crucial for correct one-hot encoding.\n",
    "        time_freq (str): Time frequency for grouping (e.g., 'M' for month,\n",
    "                         'Y' for year).\n",
    "        figure_size (tuple): Size of the matplotlib figure (width, height).\n",
    "        colormap_name (str): Name of the matplotlib colormap to use for topics.\n",
    "    \"\"\"\n",
    "    if num_total_topics is None:\n",
    "        raise ValueError(\"num_total_topics must be provided to generate the stacked area chart correctly.\")\n",
    "\n",
    "    # Merge topic assignments with timestamps\n",
    "    # Ensure indices align for a successful join.\n",
    "    temp_df = original_df_with_timestamps.copy()\n",
    "    # Join only the topic assignment column to avoid duplicate columns if original_df already has it\n",
    "    temp_df = temp_df.join(dominant_topics_info_df[[topic_assignment_col]])\n",
    "\n",
    "    # Prepare timestamp column\n",
    "    if timestamp_col not in temp_df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Timestamp column '{timestamp_col}' not found. \"\n",
    "            f\"Available columns: {temp_df.columns.tolist()}\"\n",
    "        )\n",
    "    try:\n",
    "        temp_df[timestamp_col] = pd.to_datetime(temp_df[timestamp_col], errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting column '{timestamp_col}' to datetime: {e}. Attempting with errors='coerce'.\")\n",
    "        temp_df[timestamp_col] = pd.to_datetime(temp_df[timestamp_col], errors='coerce')\n",
    "\n",
    "    # Drop rows where date conversion failed or topic assignment is missing\n",
    "    temp_df.dropna(subset=[timestamp_col, topic_assignment_col], inplace=True)\n",
    "    if temp_df.empty:\n",
    "        print(\"DataFrame is empty after handling NaNs in timestamp or topic column. Skipping stacked area plot.\")\n",
    "        return\n",
    "\n",
    "    # Extract time period for grouping\n",
    "    if time_freq == 'M':\n",
    "        temp_df['time_period'] = temp_df[timestamp_col].dt.to_period('M').dt.to_timestamp()\n",
    "    elif time_freq == 'Y':\n",
    "        temp_df['time_period'] = temp_df[timestamp_col].dt.to_period('Y').dt.to_timestamp()\n",
    "    elif time_freq == 'W':\n",
    "        temp_df['time_period'] = temp_df[timestamp_col].dt.to_period('W').dt.to_timestamp()\n",
    "    elif time_freq == 'Q':\n",
    "        temp_df['time_period'] = temp_df[timestamp_col].dt.to_period('Q').dt.to_timestamp()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported time_freq: {time_freq}. Use 'M', 'Y', 'W', or 'Q'.\")\n",
    "\n",
    "    # One-hot encode the topic assignments\n",
    "    # Ensure topic_assignment_col is integer for get_dummies\n",
    "    temp_df[topic_assignment_col] = temp_df[topic_assignment_col].astype(int)\n",
    "    topic_dummies = pd.get_dummies(temp_df[topic_assignment_col], prefix='Topic')\n",
    "\n",
    "    # Ensure all possible topic columns exist (from 0 to num_total_topics-1)\n",
    "    all_topic_cols = [f'Topic_{i}' for i in range(num_total_topics)]\n",
    "    for col in all_topic_cols:\n",
    "        if col not in topic_dummies.columns:\n",
    "            topic_dummies[col] = 0 # Add missing topic columns with 0s\n",
    "    topic_dummies = topic_dummies[all_topic_cols] # Ensure correct order and selection\n",
    "\n",
    "    # Concatenate topic dummies with the time_period column for grouping\n",
    "    temp_df_for_grouping = pd.concat([temp_df['time_period'], topic_dummies], axis=1)\n",
    "    \n",
    "    if temp_df_for_grouping.empty or 'time_period' not in temp_df_for_grouping.columns:\n",
    "        print(\"DataFrame for grouping is empty or missing 'time_period'. Skipping stacked area plot.\")\n",
    "        return\n",
    "\n",
    "    # Group by time period and sum topic counts\n",
    "    monthly_topic_counts = temp_df_for_grouping.groupby('time_period')[all_topic_cols].sum()\n",
    "    \n",
    "    if monthly_topic_counts.empty:\n",
    "        print(f\"No data to plot after grouping by 'time_period' with frequency '{time_freq}'. Skipping stacked area plot.\")\n",
    "        return\n",
    "\n",
    "    # Plot stacked area chart\n",
    "    print(\"\\nGenerating stacked area chart for topic trends...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Using a seaborn style\n",
    "    fig, ax = plt.subplots(figsize=figure_size)\n",
    "\n",
    "    try:\n",
    "        # Dynamically generate colors\n",
    "        num_plot_topics = monthly_topic_counts.shape[1]\n",
    "        if num_plot_topics == 0:\n",
    "            print(\"No topic columns to plot. Skipping stacked area chart.\")\n",
    "            return\n",
    "            \n",
    "        # Attempt to get colormap; provide fallback for very few topics\n",
    "        if num_plot_topics > 1:\n",
    "            colors = cm.get_cmap(colormap_name, num_plot_topics)(np.linspace(0, 1, num_plot_topics))\n",
    "        elif num_plot_topics == 1: # Single topic, simple color\n",
    "            colors = [cm.get_cmap(colormap_name)(0.5)] # Pick a color from the map\n",
    "        else: # No topics, should have been caught earlier\n",
    "            colors = []\n",
    "\n",
    "        # Plot each topic individually to control labels and colors\n",
    "        for i, col in enumerate(monthly_topic_counts.columns):\n",
    "            ax.fill_between(\n",
    "                monthly_topic_counts.index,\n",
    "                monthly_topic_counts[col].cumsum() if i == 0 else monthly_topic_counts.iloc[:, :i+1].sum(axis=1),\n",
    "                monthly_topic_counts.iloc[:, :i].sum(axis=1) if i > 0 else 0,\n",
    "                color=colors[i],\n",
    "                label=col,\n",
    "                linewidth=0\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate plot with colormap '{colormap_name}'. Error: {e}. Plotting with default colors.\")\n",
    "        monthly_topic_counts.plot(kind='area', stacked=True, ax=ax, alpha=0.85, linewidth=0.5)\n",
    "\n",
    "\n",
    "    ax.set_title(\"Topic Trends Over Time (Stacked Area)\", fontsize=16)\n",
    "    ax.set_xlabel(f\"Time Period ({time_freq})\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of Documents per Topic\", fontsize=14)\n",
    "\n",
    "    # Show legend clearly with colors matching the plot\n",
    "    ax.legend(title='Topics', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10, title_fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 1])  # adjust for legend\n",
    "    \n",
    "    # # Improve legend if there are many topics\n",
    "    # if num_plot_topics > 15: # Heuristic for \"many\" topics\n",
    "    #     ax.legend().set_visible(False) # Hide legend if too cluttered\n",
    "    #     print(f\"Legend hidden due to large number of topics ({num_plot_topics}).\")\n",
    "    # elif num_plot_topics > 0 :\n",
    "    #     ax.legend(title='Topics', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)\n",
    "    #     plt.tight_layout(rect=[0, 0, 0.88, 1]) # Adjust layout for legend\n",
    "    # else:\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5, axis='y')\n",
    "    plt.grid(False, which='major', linestyle='--', linewidth=0.5, axis='x') # Optional: remove vertical grid lines\n",
    "    # Save the figure\n",
    "    os.makedirs(\"lda_output\", exist_ok=True)\n",
    "    output_path = f\"lda_output/stacked_area_topics_over_time_{time_freq}.png\"\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    print(f\"Stacked area chart saved to: {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6621320d-45e1-4687-839f-8d5d0ee00871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(\n",
    "    df,\n",
    "    content_column='content',\n",
    "    is_tweet=True,\n",
    "    custom_stopwords=None,\n",
    "    min_topics=70,\n",
    "    max_topics=100,\n",
    "    step=5,\n",
    "    optimal_num_topics=None, \n",
    "    no_below=5,\n",
    "    no_above=0.5,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=50,\n",
    "    enable_time_series_plot=True,\n",
    "    timestamp_column_for_plot='date',\n",
    "    time_freq_for_plot='M',\n",
    "    normalize_time_plot=True,\n",
    "    enable_stacked_area_plot=True,\n",
    "    timestamp_col_for_stacked_plot='date',\n",
    "    topic_col_name_for_stacked_plot='dominant_topic',\n",
    "    time_freq_for_stacked_plot='M',\n",
    "    colormap_for_stacked_plot='tab20'\n",
    "):\n",
    "    \"\"\"\n",
    "    Run complete topic modeling workflow, including multiple time series plots.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing text data AND the timestamp column.\n",
    "        content_column (str): Name of the column containing text data.\n",
    "        is_tweet (bool): Whether the data consists of tweets for preprocessing.\n",
    "        custom_stopwords (list, optional): List of additional stopwords.\n",
    "        min_topics (int): Minimum number of topics for evaluation.\n",
    "        max_topics (int): Maximum number of topics for evaluation.\n",
    "        step (int): Step size for number of topics in evaluation.\n",
    "        optimal_num_topics (int, optional): If provided, skips evaluation and uses this.\n",
    "                                            If None, evaluation is performed.\n",
    "        no_below (int): Min document frequency for tokens.\n",
    "        no_above (float): Max document frequency (fraction) for tokens.\n",
    "        iterations (int): LDA model training iterations.\n",
    "        enable_time_series_plot (bool): Enable line plot for topics over time.\n",
    "        timestamp_column_for_plot (str): Timestamp column for line plot.\n",
    "        time_freq_for_plot (str): Time frequency for line plot.\n",
    "        normalize_time_plot (bool): Normalize line plot y-axis.\n",
    "        enable_stacked_area_plot (bool): Enable stacked area plot.\n",
    "        timestamp_col_for_stacked_plot (str): Timestamp col for stacked plot.\n",
    "        topic_col_name_for_stacked_plot (str): Topic ID col name for stacked plot.\n",
    "        time_freq_for_stacked_plot (str): Time frequency for stacked plot.\n",
    "        colormap_for_stacked_plot (str): Colormap for stacked plot.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model, corpus, dictionary, dominant topics, etc.\n",
    "    \"\"\"\n",
    "    # Initialize variables that will be defined in the workflow\n",
    "    final_lda_model = None\n",
    "    corpus = None\n",
    "    dictionary = None\n",
    "    processed_docs = None\n",
    "    vis = None \n",
    "\n",
    "    # Preprocessing documents \n",
    "    print(\"Preprocessing documents...\")\n",
    "    documents = df[content_column].tolist()\n",
    "    try:\n",
    "        processed_docs = preprocess_documents(documents, is_tweet=is_tweet, custom_stopwords=custom_stopwords)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during preprocessing_documents: {e}\")\n",
    "        # Depending on severity, you might want to return or raise here\n",
    "        return {\"error\": \"Preprocessing failed\"}\n",
    "\n",
    "    # Creating dictionary and corpus \n",
    "    print(\"\\nCreating dictionary and corpus...\")\n",
    "    try:\n",
    "        dictionary, corpus = create_dictionary_and_corpus(processed_docs, no_below=no_below, no_above=no_above)\n",
    "        if not dictionary or not corpus:\n",
    "            print(\"ERROR: Dictionary or corpus creation failed (returned empty).\")\n",
    "            return {\"error\": \"Dictionary/Corpus creation failed\"}\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during create_dictionary_and_corpus: {e}\")\n",
    "        return {\"error\": \"Dictionary/Corpus creation failed\"}\n",
    "\n",
    "    # Evaluating different numbers of topics \n",
    "    # This local_optimal_num_topics will be used for training.\n",
    "    # It starts as the input optimal_num_topics.\n",
    "    local_optimal_num_topics = optimal_num_topics\n",
    "\n",
    "    if local_optimal_num_topics is None:\n",
    "        topic_range = list(range(min_topics, max_topics + 1, step))\n",
    "        if not topic_range:\n",
    "            print(f\"ERROR: Topic range for evaluation is empty (min_topics={min_topics}, max_topics={max_topics}, step={step}). Using default min_topics.\")\n",
    "            local_optimal_num_topics = min_topics\n",
    "        else:\n",
    "            try:\n",
    "                coherence_scores, perplexity_scores = evaluate_topic_models(\n",
    "                    corpus=corpus,\n",
    "                    dictionary=dictionary,\n",
    "                    processed_docs=processed_docs,\n",
    "                    min_topics=min_topics,\n",
    "                    max_topics=max_topics,\n",
    "                    step=step,\n",
    "                    iterations=iterations,\n",
    "                    alpha=alpha,\n",
    "                    eta=eta,\n",
    "                )\n",
    "                plot_topic_evaluation(topic_range, coherence_scores, perplexity_scores)\n",
    "                if coherence_scores:\n",
    "                    optimal_index = coherence_scores.index(max(coherence_scores))\n",
    "                    local_optimal_num_topics = topic_range[optimal_index]\n",
    "                    print(f\"Optimal number of topics based on coherence: {local_optimal_num_topics}\")\n",
    "                else:\n",
    "                    print(\"Warning: Coherence scores list is empty. Defaulting to min_topics.\")\n",
    "                    local_optimal_num_topics = min_topics\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during topic model evaluation: {e}. Defaulting to min_topics.\")\n",
    "                local_optimal_num_topics = min_topics\n",
    "    else:\n",
    "        print(f\"\\nSkipping topic evaluation. Using pre-defined optimal_num_topics: {local_optimal_num_topics}\")\n",
    "\n",
    "    # Validate local_optimal_num_topics before training\n",
    "    if not isinstance(local_optimal_num_topics, int) or local_optimal_num_topics <= 0:\n",
    "        print(f\"ERROR: Invalid optimal_num_topics ({local_optimal_num_topics}). Must be a positive integer. Defaulting to min_topics: {min_topics}\")\n",
    "        local_optimal_num_topics = min_topics\n",
    "\n",
    "\n",
    "    # Training final LDA model\n",
    "    print(f\"\\nTraining final LDA model with {local_optimal_num_topics} topics...\")\n",
    "    try:\n",
    "        final_lda_model = train_lda_model(\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            num_topics=local_optimal_num_topics,\n",
    "            iterations=iterations,\n",
    "            alpha=alpha,\n",
    "            eta=eta\n",
    "        )\n",
    "        if final_lda_model is None:\n",
    "            print(\"ERROR: train_lda_model returned None. Model training failed.\")\n",
    "            return {\"error\": \"LDA model training failed\"}\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during final LDA model training: {e}\")\n",
    "        return {\"error\": \"LDA model training failed\"}\n",
    "\n",
    "    # Creating word clouds for topics ---\n",
    "    print(\"\\nCreating word clouds for topics...\")\n",
    "    try:\n",
    "        create_topic_wordclouds(final_lda_model, local_optimal_num_topics)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating word clouds: {e}\")\n",
    "        # This might not be a critical failure, so we can continue\n",
    "\n",
    "    # Getting dominant topic for each document\n",
    "    print(\"\\nGetting dominant topic for each document...\")\n",
    "    dominant_topics_df = pd.DataFrame() # Initialize to prevent NameError if get_dominant_topic fails\n",
    "    try:\n",
    "        dominant_topics_df = get_dominant_topic_per_document(final_lda_model, corpus)\n",
    "        if not dominant_topics_df.empty:\n",
    "            # Align index if necessary (important for joins)\n",
    "            if len(dominant_topics_df) == len(df):\n",
    "                dominant_topics_df = dominant_topics_df.set_index(df.index)\n",
    "            else:\n",
    "                print(f\"Warning: Length of dominant_topics_df ({len(dominant_topics_df)}) does not match original df ({len(df)}). Index alignment might be incorrect for joins.\")\n",
    "                # Attempt to align with the beginning of the original df's index\n",
    "                dominant_topics_df = dominant_topics_df.set_index(df.index[:len(dominant_topics_df)])\n",
    "        else:\n",
    "            print(\"Warning: get_dominant_topic_per_document returned an empty DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR getting dominant topics: {e}\")\n",
    "        # Continue if possible, plots might be skipped\n",
    "\n",
    "    # Visualizing topics with pyLDAvis \n",
    "    print(\"\\nVisualizing topics with pyLDAvis...\")\n",
    "    try:\n",
    "        print(\"\\nGenerating pyLDAvis visualization...\")\n",
    "        vis = gensimvis.prepare(final_lda_model, corpus, dictionary)\n",
    "        vis_path = os.path.join(output_dir, \"pyldavis_intertopic_map.html\")\n",
    "        pyLDAvis.save_html(vis, vis_path)\n",
    "        print(f\"Saved pyLDAvis HTML visualization to {vis_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR generating pyLDAvis HTML: {e}\")\n",
    "\n",
    "    # Plotting topics over time (LINE PLOT) \n",
    "    if enable_time_series_plot:\n",
    "        print(\"\\n Plotting topics over time (Line Plot)...\")\n",
    "        if timestamp_column_for_plot not in df.columns:\n",
    "            print(f\"Warning: Timestamp column '{timestamp_column_for_plot}' not found in df. Skipping line plot.\")\n",
    "        elif dominant_topics_df.empty:\n",
    "            print(\"Warning: dominant_topics_df is empty. Skipping line plot.\")\n",
    "        else:\n",
    "            try:\n",
    "                plot_topics_over_time(\n",
    "                    original_df=df,\n",
    "                    topics_df=dominant_topics_df,\n",
    "                    timestamp_col=timestamp_column_for_plot,\n",
    "                    topic_col_name='dominant_topic', # Assuming this col name from get_dominant_topic_per_document\n",
    "                    time_freq=time_freq_for_plot,\n",
    "                    use_normalization=normalize_time_plot\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during plot_topics_over_time (line plot): {e}\")\n",
    "    else:\n",
    "        print(\"\\n Plotting topics over time (Line Plot) skipped as per configuration.\")\n",
    "\n",
    "    # Plotting STACKED AREA topic trends \n",
    "    if enable_stacked_area_plot:\n",
    "        print(\"\\nPlotting topics over time (Stacked Area Plot)...\")\n",
    "        if timestamp_col_for_stacked_plot not in df.columns:\n",
    "            print(f\"Warning: Timestamp column '{timestamp_col_for_stacked_plot}' not found in df. Skipping stacked area plot.\")\n",
    "        elif dominant_topics_df.empty:\n",
    "            print(\"Warning: dominant_topics_df is empty. Skipping stacked area plot.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Use local_optimal_num_topics which is guaranteed to be set\n",
    "                num_topics_for_stacked_plot = local_optimal_num_topics\n",
    "                plot_stacked_topic_trends(\n",
    "                    original_df_with_timestamps=df,\n",
    "                    dominant_topics_info_df=dominant_topics_df,\n",
    "                    timestamp_col=timestamp_col_for_stacked_plot,\n",
    "                    topic_assignment_col=topic_col_name_for_stacked_plot, # Defaulted in func signature\n",
    "                    num_total_topics=num_topics_for_stacked_plot,\n",
    "                    time_freq=time_freq_for_stacked_plot,\n",
    "                    colormap_name=colormap_for_stacked_plot\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during plot_stacked_topic_trends: {e}\")\n",
    "    else:\n",
    "        print(\"\\nPlotting topics over time (Stacked Area Plot) skipped as per configuration.\")\n",
    "\n",
    "    # Return results \n",
    "    results_dict = {\n",
    "        'lda_model': final_lda_model,\n",
    "        'corpus': corpus,\n",
    "        'dictionary': dictionary,\n",
    "        'processed_docs': processed_docs,\n",
    "        'dominant_topics_df': dominant_topics_df,\n",
    "        'vis': vis,\n",
    "        'optimal_num_topics': local_optimal_num_topics # Return the actually used number of topics\n",
    "    }\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e893d931-f037-4191-8f62-13abdffbff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_wordcloud(processed_docs, output_path=\"lda_output/bigram_wordcloud.png\"):\n",
    "    \"\"\"\n",
    "    Create a word cloud from bigrams.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_docs : list of list of str\n",
    "        Tokenized documents with bigrams\n",
    "    output_path : str\n",
    "        Path to save the word cloud image\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    bigram_freq = Counter()\n",
    "    for doc in processed_docs:\n",
    "        bigrams = [token for token in doc if '_' in token]\n",
    "        bigram_freq.update(bigrams)\n",
    "\n",
    "    if not bigram_freq:\n",
    "        print(\"No bigrams found for word cloud.\")\n",
    "        return\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white'\n",
    "    ).generate_from_frequencies(bigram_freq)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Bigram WordCloud\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved bigram word cloud to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a01843c-7b32-4750-9c36-84c292b352f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge topic data with timestamps\n",
    "def plot_manual_topics_over_time(df, dominant_topics_df, timestamp_column='date', time_freq='M'):\n",
    "    if df.index.name != dominant_topics_df.index.name:\n",
    "         print(f\"Info: Index name of df is '{df.index.name}' and for dominant_topics_df is '{dominant_topics_df.index.name}'.\")\n",
    "         print(\"Joining based on index position. Ensure this is the desired behavior.\")\n",
    "         \n",
    "    # Add topic information to the original dataframe using the correct column name\n",
    "    df_merged = df.join(dominant_topics_df[['dominant_topic']])\n",
    "    \n",
    "    # Prepare the timestamp column\n",
    "    # Check if the timestamp column exists\n",
    "    if timestamp_column not in df_merged.columns:\n",
    "        raise ValueError(f\"Timestamp column '{timestamp_column}' not found in the DataFrame 'df_merged'. Please check the 'timestamp_column' variable and ensure it's present in your original 'df'.\")\n",
    "    \n",
    "    # Convert timestamp column to datetime objects (handle potential errors)\n",
    "    try:\n",
    "        df_merged[timestamp_column] = pd.to_datetime(df_merged[timestamp_column])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting column '{timestamp_column}' to datetime: {e}\")\n",
    "        print(\"Attempting conversion with error coercion (invalid dates become NaT)...\")\n",
    "        df_merged[timestamp_column] = pd.to_datetime(df_merged[timestamp_column], errors='coerce')\n",
    "    \n",
    "    # Drop rows where date conversion failed or where the dominant topic is missing\n",
    "    df_merged.dropna(subset=[timestamp_column, 'dominant_topic'], inplace=True)\n",
    "    \n",
    "    # Ensure Dominant_Topic is integer or suitable for grouping\n",
    "    df_merged['dominant_topic'] = df_merged['dominant_topic'].astype(int) # Corrected column name\n",
    "    \n",
    "    \n",
    "    # Group by time and topic\n",
    "    # Set timestamp as index for resampling\n",
    "    df_merged.set_index(timestamp_column, inplace=True)\n",
    "    \n",
    "    # Group by the chosen frequency and dominant topic, then count occurrences\n",
    "    \n",
    "    topics_over_time = df_merged.groupby([pd.Grouper(freq=time_freq), 'dominant_topic']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Plotting using Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for topic in topics_over_time.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=topics_over_time.index,\n",
    "            y=topics_over_time[topic],\n",
    "            mode='lines+markers',\n",
    "            name=f'Topic {topic}'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Topics Over Time ({time_freq} Frequency)',\n",
    "        xaxis_title='Time Period',\n",
    "        yaxis_title='Number of Documents',\n",
    "        legend_title='Topics',\n",
    "        template='plotly_white',\n",
    "        hovermode='x'\n",
    "    )\n",
    "\n",
    "    # Improve x-axis formatting for dates\n",
    "    if time_freq == 'M':\n",
    "        fig.update_xaxes(tickformat='%Y-%m')\n",
    "    elif time_freq == 'Y':\n",
    "        fig.update_xaxes(tickformat='%Y')\n",
    "    elif time_freq == 'W':\n",
    "        fig.update_xaxes(tickformat='%Y-%W')\n",
    "    elif time_freq == 'Q':\n",
    "        fig.update_xaxes(tickformat='%Y-Q%q')\n",
    "\n",
    "    # Save as interactive HTML\n",
    "    output_path = f\"lda_output/manual_topics_over_time_{time_freq}.html\"\n",
    "    pio.write_html(fig, file=output_path, auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8297456d-adbb-4228-b994-ba2d91b33566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_lda_model_results(lda_model, dictionary, dominant_topics_df, output_dir=\"lda_output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save LDA model\n",
    "    model_path = os.path.join(output_dir, \"lda_model\")\n",
    "    lda_model.save(model_path)\n",
    "    print(f\"Saved LDA model to {model_path}\")\n",
    "\n",
    "    # Save dictionary as a structured CSV\n",
    "    dict_tokens = [{\"ID\": token_id, \"Word\": word, \"Frequency\": dictionary.dfs.get(token_id, 0)}\n",
    "                   for token_id, word in dictionary.iteritems()]\n",
    "    dict_df = pd.DataFrame(dict_tokens)\n",
    "    dict_df.sort_values(by=\"Frequency\", ascending=False, inplace=True)\n",
    "    \n",
    "    dict_path = os.path.join(output_dir, \"lda_dictionary.csv\")\n",
    "    dict_df.to_csv(dict_path, index=False)\n",
    "    print(f\"Saved structured dictionary to {dict_path}\")\n",
    "\n",
    "    # Save dominant topics assignment (if available)\n",
    "    if not dominant_topics_df.empty:\n",
    "\n",
    "        # Save top topic terms\n",
    "        top_terms = []\n",
    "        for topic_id in range(lda_model.num_topics):\n",
    "            for term, weight in lda_model.show_topic(topic_id, topn=10):\n",
    "                top_terms.append({\n",
    "                    'Topic': topic_id,\n",
    "                    'Term': term,\n",
    "                    'Weight': round(weight, 4)\n",
    "                })\n",
    "    \n",
    "        terms_df = pd.DataFrame(top_terms)\n",
    "        terms_path = os.path.join(output_dir, \"lda_topic_terms.csv\")\n",
    "        terms_df.to_csv(terms_path, index=False)\n",
    "        print(f\"Saved top topic terms to {terms_path}\")\n",
    "    \n",
    "        # Save dominant topic assignment\n",
    "        dom_path = os.path.join(output_dir, \"lda_dominant_topics.csv\")\n",
    "        dominant_topics_df.to_csv(dom_path, index=False)\n",
    "        print(f\"Saved dominant topic assignments to {dom_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af4107f5-babb-4889-a5bb-ee32a59f45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_lda_summary(lda_model, dominant_topics_df, content_column='content', output_path='lda_topic_info.csv',output_dir=\"lda_output\"):\n",
    "    \"\"\"\n",
    "    Create a summary CSV file of LDA topics with topic ID, count, top terms, and a representative document.\n",
    "\n",
    "    Args:\n",
    "        lda_model (gensim.models.LdaModel): Trained LDA model.\n",
    "        dominant_topics_df (pd.DataFrame): DataFrame with dominant topic assignments.\n",
    "        content_column (str): Name of the column with original text (must be present in dominant_topics_df).\n",
    "        output_path (str): Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if 'dominant_topic' not in dominant_topics_df.columns:\n",
    "        print(\"Error: dominant_topic column not found in dominant_topics_df\")\n",
    "        return\n",
    "\n",
    "    topic_counts = dominant_topics_df['dominant_topic'].value_counts().sort_index()\n",
    "\n",
    "    topic_info = []\n",
    "    for topic_id in topic_counts.index:\n",
    "        # Name: use underscore-separated top 3 words\n",
    "        top_words = [word for word, _ in lda_model.show_topic(topic_id, topn=3)]\n",
    "        topic_name = \"_\".join(top_words)\n",
    "\n",
    "        # Full representation: top 10 words\n",
    "        full_representation = [word for word, _ in lda_model.show_topic(topic_id, topn=10)]\n",
    "\n",
    "        topic_info.append({\n",
    "            \"Topic\": topic_id,\n",
    "            \"Count\": topic_counts[topic_id],\n",
    "            \"Name\": topic_name,\n",
    "            \"Representation\": full_representation,\n",
    "        })\n",
    "\n",
    "    topic_info_df = pd.DataFrame(topic_info)\n",
    "    \n",
    "    terms_path = os.path.join(output_dir, output_path)\n",
    "    topic_info_df.to_csv(terms_path, index=False)\n",
    "    \n",
    "    print(f\"LDA topic summary exported to{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9bc8cba-c800-46d7-ac08-1f41940a3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_lda_run(model, dataset_name, optimal_num_topics, coherence_score, log_path='lda_modeling_log.csv'):\n",
    "    \"\"\"\n",
    "    Log the LDA run into a CSV file, including topic count and coherence score.\n",
    "\n",
    "    Args:\n",
    "        model (LdaModel): The trained LDA model.\n",
    "        dataset_name (str): Identifier of the dataset.\n",
    "        optimal_num_topics (int): The number of topics used.\n",
    "        coherence_score (float): Coherence score.\n",
    "        log_path (str): Path to save the CSV log.\n",
    "    \"\"\"\n",
    "    run_data = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'dataset_name': dataset_name,\n",
    "        'model_name': 'LDA (gensim)',\n",
    "        'parameters': f\"min_topics={config['min_topics']}, max_topics={config['max_topics']}, \"\n",
    "                      f\"step={config['step']}, no_below={config['no_below']}, no_above={config['no_above']}, \"\n",
    "                      f\"optimal_num_topics={config['optimal_num_topics']}, iterations={config['iterations']}, \"\n",
    "                      f\"alpha={config['alpha']}, eta={config['eta']}\",\n",
    "        'n_topics': optimal_num_topics,\n",
    "        'coherence': round(coherence_score, 4)\n",
    "    }\n",
    "\n",
    "    df_run = pd.DataFrame([run_data])\n",
    "    full_log_path = os.path.join(output_dir, log_path)\n",
    "    if os.path.exists(full_log_path):\n",
    "        df_run.to_csv(full_log_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_run.to_csv(full_log_path, mode='w', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9b2de66-2064-48a3-bca6-9a2f0df8cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topic_assignment_with_labels(df, dominant_topics_df, content_column='content', output_path='lda_output/tweet_topic_label_mapping.csv'):\n",
    "\n",
    "    # Check required columns\n",
    "    if content_column not in df.columns:\n",
    "        raise ValueError(f\"'{content_column}' column not found in the DataFrame.\")\n",
    "    \n",
    "    if dominant_topics_df.empty:\n",
    "        raise ValueError(\"dominant_topics_df is empty.\")\n",
    "    \n",
    "    # Align indices if needed\n",
    "    if not df.index.equals(dominant_topics_df.index):\n",
    "        print(\"Aligning indices between df and dominant_topics_df...\")\n",
    "        dominant_topics_df = dominant_topics_df.set_index(df.index[:len(dominant_topics_df)])\n",
    "\n",
    "    # Merge and save\n",
    "    merged_df = df[[content_column]].copy()\n",
    "    merged_df = merged_df.join(dominant_topics_df[['dominant_topic']])\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Tweet-topic-label mapping saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c8ac3aa-4def-4792-9ed4-ab5663a7d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_topic_distribution_from_db_and_csv(\n",
    "    db_path,\n",
    "    topics_file,\n",
    "    topic_info_file='lda_output/lda_dominant_topics.csv',\n",
    "    output_path='lda_output/topic_distribution_by_label.png',\n",
    "    top_n=13\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot distribution of topics by label using tweet-topic mapping, DB labels, and topic names from CSV.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database.\n",
    "        topics_file (str): Path to the CSV file with tweet-to-topic mapping (must include 'content' and 'dominant_topic').\n",
    "        topic_info_file (str): CSV file that maps dominant_topic IDs to top_words.\n",
    "        output_path (str): File path to save the plot.\n",
    "        top_n (int): Number of top topics to display based on total proportions.\n",
    "    \"\"\"\n",
    "    import sqlite3\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "\n",
    "    # Load topic-to-top_words mapping\n",
    "    topic_info_df = pd.read_csv(topic_info_file)\n",
    "    topic_name_map = {\n",
    "        row['dominant_topic']: row['top_words']\n",
    "        for _, row in topic_info_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    query = \"\"\"\n",
    "    SELECT p.author, a.label, p.content\n",
    "    FROM posts p\n",
    "    JOIN authors a ON p.author = a.author_screen_name;\n",
    "    \"\"\"\n",
    "    posts_df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Load tweet-topic mapping\n",
    "    topics_df = pd.read_csv(topics_file)\n",
    "\n",
    "    # Merge on content\n",
    "    merged_df = pd.merge(posts_df, topics_df, left_on='content', right_on='content', how='inner')\n",
    "\n",
    "    # Group by label and topic\n",
    "    grouped_df = merged_df.groupby(['label', 'dominant_topic']).size().reset_index(name='Count')\n",
    "    grouped_df['Proportion'] = grouped_df.groupby('label')['Count'].transform(lambda x: x / x.sum())\n",
    "\n",
    "    # Pivot for plotting\n",
    "    pivot_df = grouped_df.pivot(index='dominant_topic', columns='label', values='Proportion').fillna(0)\n",
    "    pivot_df['total'] = pivot_df.sum(axis=1)\n",
    "    pivot_df = pivot_df.sort_values(by='total', ascending=False).head(top_n)\n",
    "    pivot_df.drop(columns='total', inplace=True)\n",
    "\n",
    "    # Get topic names from topic_name_map\n",
    "    topic_labels = [topic_name_map.get(topic_id, f\"Topic {topic_id}\") for topic_id in pivot_df.index]\n",
    "\n",
    "    # Plot\n",
    "    ax = pivot_df.plot(kind='bar', figsize=(18, 8), width=0.8)\n",
    "    ax.set_title(\"Distribution of Tweets by Topic Category\", fontsize=16, weight='bold')\n",
    "    ax.set_xlabel(\"Topics\", fontsize=14)\n",
    "    ax.set_ylabel(\"Proportion of Tweets\", fontsize=14)\n",
    "    ax.set_xticklabels(topic_labels, rotation=45, ha='right')\n",
    "    ax.legend(title=\"Categories\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    \n",
    "    # Adding percentage values above each bar\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%', padding=3, fontsize=10,\n",
    "                     labels=[f\"{val*100:.1f}%\" for val in container.datavalues])\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43457fac-52e3-4259-948a-c78da4bc8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Wedge, Rectangle\n",
    "from wordcloud import WordCloud\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_sunburst_for_top_topics(\n",
    "    db_path,\n",
    "    topics_file,\n",
    "    topic_info_file,\n",
    "    output_path='lda_output/sunburst_chart.png',\n",
    "    top_n=6\n",
    "):\n",
    "    def draw_sunburst_donut(ax, group1_yes, group1_no, group2_yes, group2_no, top_words, title, colors):\n",
    "        group1_total = group1_yes + group1_no\n",
    "        group2_total = group2_yes + group2_no\n",
    "\n",
    "        # Inner ring (group1)\n",
    "        group1_yes_angle = group1_yes / group1_total * 360 if group1_total else 0\n",
    "        inner_wedges = [\n",
    "            (0, group1_yes_angle, colors['group1_yes']),\n",
    "            (group1_yes_angle, 360, colors['group1_no'])\n",
    "        ]\n",
    "        for start, end, color in inner_wedges:\n",
    "            ax.add_patch(Wedge((0, 0), 1.0, start, end, width=0.3, color=color))\n",
    "\n",
    "        # Outer ring (group2)\n",
    "        group2_yes_angle = group2_yes / group2_total * 360 if group2_total else 0\n",
    "        outer_wedges = [\n",
    "            (0, group2_yes_angle, colors['group2_yes']),\n",
    "            (group2_yes_angle, 360, colors['group2_no'])\n",
    "        ]\n",
    "        for start, end, color in outer_wedges:\n",
    "            ax.add_patch(Wedge((0, 0), 1.3, start, end, width=0.3, color=color))\n",
    "\n",
    "        # Word cloud\n",
    "        wc = WordCloud(width=300, height=300, background_color='white').generate(top_words)\n",
    "        ax.imshow(wc, extent=[-0.5, 0.5, -0.5, 0.5], zorder=10)\n",
    "\n",
    "        # Title\n",
    "        ax.text(0, 1.4, title, ha='center', va='bottom', fontsize=10, wrap=True)\n",
    "        ax.set_xlim(-1.5, 1.5)\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Load data\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    posts_df = pd.read_sql_query(\"\"\"\n",
    "        SELECT p.content, a.label, p.author\n",
    "        FROM posts p\n",
    "        JOIN authors a ON p.author = a.author_screen_name;\n",
    "    \"\"\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    topics_df = pd.read_csv(topics_file)\n",
    "    topic_info_df = pd.read_csv(topic_info_file)\n",
    "    topic_words_map = topic_info_df.set_index('dominant_topic')['top_words'].to_dict()\n",
    "\n",
    "    merged_df = pd.merge(posts_df, topics_df, on='content', how='inner')\n",
    "    top_topics = merged_df['dominant_topic'].value_counts().nlargest(top_n).index.tolist()\n",
    "\n",
    "    # Extract unique labels\n",
    "    unique_labels = merged_df['label'].unique()\n",
    "    unique_labels = [x for x in unique_labels if x not in [None, \"None\"]]\n",
    "    if len(unique_labels) != 2:\n",
    "        raise ValueError(f\"Expected exactly 2 unique labels, got: {unique_labels}\")\n",
    "    group1_label, group2_label = unique_labels\n",
    "\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        'group1_yes': '#1f77b4',   # blue\n",
    "        'group1_no': '#2ca02c',    # green\n",
    "        'group2_yes': '#9467bd',   # purple\n",
    "        'group2_no': '#d62728'     # red\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, topic_id in enumerate(top_topics):\n",
    "        df = merged_df.copy()\n",
    "        df['participated'] = df['dominant_topic'] == topic_id\n",
    "\n",
    "        unique_authors = df.drop_duplicates(subset='author')\n",
    "\n",
    "        group1_yes = len(unique_authors[(unique_authors['label'] == group1_label) & (unique_authors['participated'])])\n",
    "        group1_no  = len(unique_authors[(unique_authors['label'] == group1_label) & (~unique_authors['participated'])])\n",
    "        group2_yes = len(unique_authors[(unique_authors['label'] == group2_label) & (unique_authors['participated'])])\n",
    "        group2_no  = len(unique_authors[(unique_authors['label'] == group2_label) & (~unique_authors['participated'])])\n",
    "\n",
    "        top_words = topic_words_map.get(topic_id, '')\n",
    "        draw_sunburst_donut(\n",
    "            axes[i], group1_yes, group1_no, group2_yes, group2_no, top_words, top_words, colors\n",
    "        )\n",
    "\n",
    "    # Legend\n",
    "    legend_ax = fig.add_axes([0.1, 0.05, 0.8, 0.05])\n",
    "    legend_ax.set_axis_off()\n",
    "    items = [\n",
    "        (0.1, colors['group1_yes'], f'{group1_label} Participated'),\n",
    "        (0.3, colors['group1_no'], f'{group1_label} Not Participated'),\n",
    "        (0.5, colors['group2_yes'], f'{group2_label} Participated'),\n",
    "        (0.7, colors['group2_no'], f'{group2_label} Not Participated'),\n",
    "    ]\n",
    "    for pos, color, label in items:\n",
    "        legend_ax.add_patch(Rectangle((pos, 0.2), 0.03, 0.6, color=color))\n",
    "        legend_ax.text(pos + 0.04, 0.5, label, va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Sunburst donut chart saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf2d9ba-fa84-479c-9696-69bf13783a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_merge_topics(topic1, topic2):\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert in topic modeling and keyword clustering.\n",
    "\n",
    "Your task is to decide whether the following two topic names refer to the **same core topic** or represent **closely related concepts**.\n",
    "\n",
    "Please respond with **\"yes\"** if the two topics should be merged  for example, if they are:\n",
    "- synonyms,\n",
    "- strongly related,\n",
    "- overlapping in meaning,\n",
    "- or commonly used to describe the same type of content.\n",
    "\n",
    "Respond with **\"no\"** if they describe clearly **different topics** or represent **distinct themes**.\n",
    "\n",
    "Your answer should be **only** \"yes\" or \"no\"  no explanations or extra text.\n",
    "\"\"\"\n",
    "    user_prompt = f\"\"\"Topic 1: {topic1}\n",
    "Topic 2: {topic2}\n",
    "\n",
    "Do these two topics represent the same topic and should be merged?\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=1.0\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        return answer == \"yes\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pair ({topic1}, {topic2}): {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_merge_topics(topic_pairs):\n",
    "    merged_groups = []\n",
    "    \n",
    "    for topic1, topic2 in tqdm(topic_pairs, desc=\"Checking topic pairs\"):\n",
    "        if should_merge_topics(topic1, topic2):\n",
    "            # Merge into existing group if present\n",
    "            found = False\n",
    "            for group in merged_groups:\n",
    "                if topic1 in group or topic2 in group:\n",
    "                    group.update([topic1, topic2])\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                merged_groups.append(set([topic1, topic2]))\n",
    "    return merged_groups\n",
    "\n",
    "def generate_group_name_from_topics(topics):\n",
    "    user_prompt = (\n",
    "        \"You are an expert in topic modeling.\\n\"\n",
    "        \"Given the following topic names:\\n\" +\n",
    "        '\\n'.join(f\"- {t}\" for t in topics) +\n",
    "        \"\\nSuggest a concise 23 word name summarizing the shared theme.\\n\"\n",
    "        \"Output ONLY the name  no punctuation or explanation.\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a topic naming assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        top_p=1.0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_topic_pairs_from_csv(file_path, topic_name_column='Name'):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and generates all unique topic pairs from a column containing topic names.\n",
    "    Returns a list of (topic1, topic2) tuples.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if topic_name_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{topic_name_column}' not found in CSV.\")\n",
    "\n",
    "    df['clean_topic_name'] = df[topic_name_column].astype(str).str.replace(r'^\\d+_', '', regex=True)\n",
    "    \n",
    "    topic_names = df['clean_topic_name'].dropna().unique().tolist()\n",
    "    topic_pairs = list(combinations(topic_names, 2))\n",
    "\n",
    "    return topic_pairs\n",
    "\n",
    "def assign_merged_topic_names(merged_groups):\n",
    "    \"\"\"\n",
    "    Assigns LLM-generated names to merged topic groups.\n",
    "    Returns:\n",
    "        - topic_to_group_name: {original_topic_name: descriptive_name}\n",
    "    \"\"\"\n",
    "    topic_to_group_name = {}\n",
    "\n",
    "    for idx, group in enumerate(merged_groups):\n",
    "        group_id = f\"Group_{idx+1}\"\n",
    "        group = sorted(group)\n",
    "        try:\n",
    "            name = generate_group_name_from_topics(group)\n",
    "        except Exception as e:\n",
    "            print(f\"Error naming group {group_id}: {e}\")\n",
    "            name = group_id\n",
    "\n",
    "        for topic in group:\n",
    "            topic_to_group_name[topic] = name\n",
    "\n",
    "    return topic_to_group_name\n",
    "\n",
    "def apply_merged_names_to_tweets(mapping_file_path, topic_to_group_name, output_path):\n",
    "    \"\"\"\n",
    "    Loads a tweet-topic mapping file, replaces the topic names with merged group names, and saves to new CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(mapping_file_path)\n",
    "    df['clean_topic'] = df['dominant_topic'].astype(str).str.replace(r'^\\d+_', '', regex=True)\n",
    "    df['merged_topic'] = df['clean_topic'].map(topic_to_group_name)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved updated tweet-topic mapping with merged names to: {output_path}\")\n",
    "\n",
    "def merge_topics(file_path):\n",
    "    topic_pairs = extract_topic_pairs_from_csv(file_path)\n",
    "    merged_groups = generate_merge_topics(topic_pairs)\n",
    "    topic_to_group_name = assign_merged_topic_names(merged_groups)\n",
    "    apply_merged_names_to_tweets('lda_output/tweet_topic_label_mapping.csv', topic_to_group_name, 'lda_output/tweet_merged_topic_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bef40bd-4332-4ca7-bf81-ee633106d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849257/15938557.py:40: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[timestamp_column] = pd.to_datetime(df[timestamp_column], errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 records from posts\n",
      "Preprocessing documents...\n",
      "Processing 10000 documents...\n",
      "Processed 1000/10000 documents\n",
      "Processed 2000/10000 documents\n",
      "Processed 3000/10000 documents\n",
      "Processed 4000/10000 documents\n",
      "Processed 5000/10000 documents\n",
      "Processed 6000/10000 documents\n",
      "Processed 7000/10000 documents\n",
      "Processed 8000/10000 documents\n",
      "Processed 9000/10000 documents\n",
      "Finished processing. 7315 documents retained after preprocessing.\n",
      "\n",
      "Creating dictionary and corpus...\n",
      "Dictionary size: 3592\n",
      "Corpus size: 7315\n",
      "Evaluating model with 10 topics...\n",
      "Coherence score: 0.4626\n",
      "Perplexity: -6.5440\n",
      "--------------------------------------------------\n",
      "Evaluating model with 15 topics...\n",
      "Coherence score: 0.5017\n",
      "Perplexity: -8.1166\n",
      "--------------------------------------------------\n",
      "Evaluating model with 20 topics...\n",
      "Coherence score: 0.4797\n",
      "Perplexity: -9.2390\n",
      "--------------------------------------------------\n",
      "Evaluating model with 25 topics...\n",
      "Coherence score: 0.5007\n",
      "Perplexity: -9.9365\n",
      "--------------------------------------------------\n",
      "Evaluating model with 30 topics...\n",
      "Coherence score: 0.4899\n",
      "Perplexity: -10.6122\n",
      "--------------------------------------------------\n",
      "Evaluating model with 35 topics...\n",
      "Coherence score: 0.4939\n",
      "Perplexity: -11.3096\n",
      "--------------------------------------------------\n",
      "Evaluating model with 40 topics...\n",
      "Coherence score: 0.5164\n",
      "Perplexity: -12.0010\n",
      "--------------------------------------------------\n",
      "Evaluating model with 45 topics...\n",
      "Coherence score: 0.5212\n",
      "Perplexity: -12.6877\n",
      "--------------------------------------------------\n",
      "Evaluating model with 50 topics...\n",
      "Coherence score: 0.5250\n",
      "Perplexity: -13.4018\n",
      "--------------------------------------------------\n",
      "Evaluating model with 55 topics...\n",
      "Coherence score: 0.5372\n",
      "Perplexity: -14.1306\n",
      "--------------------------------------------------\n",
      "Evaluating model with 60 topics...\n",
      "Coherence score: 0.5466\n",
      "Perplexity: -14.9156\n",
      "--------------------------------------------------\n",
      "Optimal number of topics based on coherence: 60\n",
      "\n",
      "Training final LDA model with 60 topics...\n",
      "\n",
      "Creating word clouds for topics...\n",
      "\n",
      "Getting dominant topic for each document...\n",
      "Warning: Length of dominant_topics_df (7315) does not match original df (10000). Index alignment might be incorrect for joins.\n",
      "\n",
      "Visualizing topics with pyLDAvis...\n",
      "\n",
      "Generating pyLDAvis visualization...\n",
      "Saved pyLDAvis HTML visualization to lda_output/pyldavis_intertopic_map.html\n",
      "\n",
      " Plotting topics over time (Line Plot)...\n",
      "ERROR during plot_topics_over_time (line plot): name 'plot_topics_over_time' is not defined\n",
      "\n",
      "Plotting topics over time (Stacked Area Plot)...\n",
      "\n",
      "Generating stacked area chart for topic trends...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849257/356343406.py:61: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  temp_df['time_period'] = temp_df[timestamp_col].dt.to_period('M').dt.to_timestamp()\n",
      "/tmp/ipykernel_849257/356343406.py:152: UserWarning: First parameter to grid() is false, but line properties are supplied. The grid will be enabled.\n",
      "  plt.grid(False, which='major', linestyle='--', linewidth=0.5, axis='x') # Optional: remove vertical grid lines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked area chart saved to: lda_output/stacked_area_topics_over_time_M.png\n",
      "\n",
      "--- Analysis Complete ---\n",
      "Optimal number of topics found/used: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849257/851691232.py:36: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  topics_over_time = df_merged.groupby([pd.Grouper(freq=time_freq), 'dominant_topic']).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LDA model to lda_output/lda_model\n",
      "Saved structured dictionary to lda_output/lda_dictionary.csv\n",
      "Saved top topic terms to lda_output/lda_topic_terms.csv\n",
      "Saved dominant topic assignments to lda_output/lda_dominant_topics.csv\n",
      "LDA topic summary exported tolda_topic_info.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/xdg-open: line 881: x-www-browser: command not found\n",
      "/usr/bin/xdg-open: line 881: firefox: command not found\n",
      "/usr/bin/xdg-open: line 881: iceweasel: command not found\n",
      "/usr/bin/xdg-open: line 881: seamonkey: command not found\n",
      "/usr/bin/xdg-open: line 881: mozilla: command not found\n",
      "/usr/bin/xdg-open: line 881: epiphany: command not found\n",
      "/usr/bin/xdg-open: line 881: konqueror: command not found\n",
      "/usr/bin/xdg-open: line 881: chromium: command not found\n",
      "/usr/bin/xdg-open: line 881: chromium-browser: command not found\n",
      "/usr/bin/xdg-open: line 881: google-chrome: command not found\n",
      "/usr/bin/xdg-open: line 881: www-browser: command not found\n",
      "/usr/bin/xdg-open: line 881: links2: command not found\n",
      "/usr/bin/xdg-open: line 881: elinks: command not found\n",
      "/usr/bin/xdg-open: line 881: links: command not found\n",
      "/usr/bin/xdg-open: line 881: lynx: command not found\n",
      "/usr/bin/xdg-open: line 881: w3m: command not found\n",
      "xdg-open: no method available for opening 'file:///sise/4-year-ise-proj/Ise4thYear/puzis-twitterAnalysisGeneric-2025/lda_output/manual_topics_over_time_M.html'\n",
      "/usr/bin/xdg-open: line 881: w3m: command not found\n",
      "xdg-open: no method available for opening 'file:///sise/4-year-ise-proj/Ise4thYear/puzis-twitterAnalysisGeneric-2025/lda_output/manual_topics_over_time_M.html'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning indices between df and dominant_topics_df...\n",
      "Tweet-topic-label mapping saved to: lda_output/tweet_topic_label_mapping.csv\n",
      "Plot saved to: lda_output/topic_distribution_by_label.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849257/49491122.py:112: UserWarning:\n",
      "\n",
      "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunburst donut chart saved to: lda_output/sunburst_chart.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'combinations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 117\u001b[0m\n\u001b[1;32m    103\u001b[0m plot_topic_distribution_from_db_and_csv(\n\u001b[1;32m    104\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    105\u001b[0m     topics_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/tweet_topic_label_mapping.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    106\u001b[0m     topic_info_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/lda_dominant_topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    107\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/topic_distribution_by_label.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m plot_sunburst_for_top_topics(\n\u001b[1;32m    111\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    112\u001b[0m     topics_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/tweet_topic_label_mapping.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    113\u001b[0m     topic_info_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/lda_dominant_topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    114\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/sunburst_chart.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    115\u001b[0m )\n\u001b[0;32m--> 117\u001b[0m \u001b[43mmerge_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlda_output/lda_topic_info.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m plot_topic_distribution_from_db_and_csv(\n\u001b[1;32m    119\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    120\u001b[0m     topics_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/tweet_merged_topic_mapping.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    121\u001b[0m     topic_info_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/lda_dominant_topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    122\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/topic_distribution_by_label_after_merge.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m plot_sunburst_for_top_topics(\n\u001b[1;32m    126\u001b[0m     db_path\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    127\u001b[0m     topics_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/tweet_merged_topic_mapping.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    128\u001b[0m     topic_info_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/lda_dominant_topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    129\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_output/sunburst_chart_after_merge.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    130\u001b[0m )\n",
      "Cell \u001b[0;32mIn[29], line 125\u001b[0m, in \u001b[0;36mmerge_topics\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_topics\u001b[39m(file_path):\n\u001b[0;32m--> 125\u001b[0m     topic_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mextract_topic_pairs_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     merged_groups \u001b[38;5;241m=\u001b[39m generate_merge_topics(topic_pairs)\n\u001b[1;32m    127\u001b[0m     topic_to_group_name \u001b[38;5;241m=\u001b[39m assign_merged_topic_names(merged_groups)\n",
      "Cell \u001b[0;32mIn[29], line 88\u001b[0m, in \u001b[0;36mextract_topic_pairs_from_csv\u001b[0;34m(file_path, topic_name_column)\u001b[0m\n\u001b[1;32m     85\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_topic_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[topic_name_column]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m topic_names \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_topic_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 88\u001b[0m topic_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mcombinations\u001b[49m(topic_names, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m topic_pairs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combinations' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = load_config()\n",
    "    df = load_data(\n",
    "        db_path=config['database_path'],\n",
    "        table_name='posts',\n",
    "        content_column='content',\n",
    "        limit=config['limit']\n",
    "    )\n",
    "    custom_stopwords = config.get('custom_stopwords', [])\n",
    "\n",
    "    # Parameters for the LINE PLOT topics over time\n",
    "    enable_line_plot = config.get('enable_time_series_plot', True)\n",
    "    ts_column_line = config.get('timestamp_column_name', 'date') # Assuming 'date' from previous\n",
    "    line_plot_freq = config.get('plot_time_frequency', 'M')\n",
    "    line_plot_norm = config.get('plot_normalize_time_series', True)\n",
    "\n",
    "    # Parameters for the STACKED AREA topics over time plot\n",
    "    enable_stack_plot = config.get('enable_stacked_area_plot', True) # Default to True\n",
    "    # Can reuse timestamp col if same, or define a new one in config\n",
    "    ts_column_stack = config.get('timestamp_column_for_stacked_plot', ts_column_line)\n",
    "    stack_plot_freq = config.get('stacked_plot_time_frequency', 'M')\n",
    "    stack_plot_colormap = config.get('stacked_plot_colormap', 'tab20')\n",
    "\n",
    "\n",
    "    # Run topic modeling\n",
    "    results = run_topic_modeling(\n",
    "        df,\n",
    "        content_column='content',\n",
    "        is_tweet=True,\n",
    "        custom_stopwords=custom_stopwords,\n",
    "        min_topics=config['min_topics'],\n",
    "        max_topics=config['max_topics'],\n",
    "        step=config['step'],\n",
    "        optimal_num_topics=config.get('optimal_num_topics', None),\n",
    "        no_below=config['no_below'],\n",
    "        no_above=config['no_above'],\n",
    "        iterations=config.get('iterations', 50),\n",
    "        # Line plot params\n",
    "        enable_time_series_plot=enable_line_plot,\n",
    "        timestamp_column_for_plot=ts_column_line,\n",
    "        time_freq_for_plot=line_plot_freq,\n",
    "        normalize_time_plot=line_plot_norm,\n",
    "        # Stacked area plot params\n",
    "        enable_stacked_area_plot=enable_stack_plot,\n",
    "        timestamp_col_for_stacked_plot=ts_column_stack,\n",
    "        # topic_col_name_for_stacked_plot is defaulted in run_topic_modeling to 'dominant_topic'\n",
    "        time_freq_for_stacked_plot=stack_plot_freq,\n",
    "        colormap_for_stacked_plot=stack_plot_colormap\n",
    "    )\n",
    "\n",
    "    # Access results\n",
    "    lda_model = results['lda_model']\n",
    "    dominant_topics_df = results['dominant_topics_df']\n",
    "    optimal_num_topics = results.get('optimal_num_topics', 'N/A')\n",
    "\n",
    "    print(f\"\\n--- Analysis Complete ---\")\n",
    "    print(f\"Optimal number of topics found/used: {optimal_num_topics}\")\n",
    "\n",
    "    # Show the distribution of dominant topics\n",
    "    if not dominant_topics_df.empty:\n",
    "        topic_distribution = dominant_topics_df['dominant_topic'].value_counts().sort_values(ascending=False)\n",
    "        topic_distribution = topic_distribution.head(10)\n",
    "\n",
    "        # Get topic names (e.g., top 3 words per topic)\n",
    "        topic_names = {}\n",
    "        for topic_id in topic_distribution.index:\n",
    "            words = [word for word, _ in lda_model.show_topic(topic_id, topn=3)]\n",
    "            topic_names[topic_id] = ', '.join(words)\n",
    "    \n",
    "        # Prepare data for plotting\n",
    "        topic_ids = list(topic_distribution.index)\n",
    "        topic_counts = topic_distribution.values\n",
    "        topic_labels = [topic_names[tid] for tid in topic_ids]\n",
    "        topic_labels = [topic_names[tid].replace(' ', '\\n') for tid in topic_ids]\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.barplot(x=topic_labels, y=topic_counts)\n",
    "        plt.title('Distribution of Dominant Topics')\n",
    "        plt.xlabel('Topic Name')\n",
    "        plt.ylabel('Number of Documents')\n",
    "        output_path = os.path.join(output_dir, \"dominant_topic_distribution.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        plot_manual_topics_over_time(df, dominant_topics_df, timestamp_column=ts_column_line, time_freq=line_plot_freq)\n",
    "        export_lda_model_results(lda_model, dictionary=results['dictionary'], dominant_topics_df=dominant_topics_df)\n",
    "        export_lda_summary(lda_model, dominant_topics_df, content_column='content', output_path=\"lda_topic_info.csv\")\n",
    "        lda_coherence = compute_coherence_score(\n",
    "            model=lda_model,\n",
    "            corpus=results['corpus'],\n",
    "            dictionary=results['dictionary'],\n",
    "            processed_docs=results['processed_docs']\n",
    "        )\n",
    "\n",
    "        save_topic_assignment_with_labels(\n",
    "            df=df,\n",
    "            dominant_topics_df=results['dominant_topics_df'],\n",
    "            content_column='content',  \n",
    "            output_path='lda_output/tweet_topic_label_mapping.csv'\n",
    "        )\n",
    "\n",
    "\n",
    "        plot_topic_distribution_from_db_and_csv(\n",
    "            db_path=config['database_path'], \n",
    "            topics_file='lda_output/tweet_topic_label_mapping.csv',\n",
    "            topic_info_file='lda_output/lda_dominant_topics.csv',\n",
    "            output_path='lda_output/topic_distribution_by_label.png'\n",
    "        )\n",
    "\n",
    "        plot_sunburst_for_top_topics(\n",
    "            db_path=config['database_path'], \n",
    "            topics_file='lda_output/tweet_topic_label_mapping.csv',\n",
    "            topic_info_file='lda_output/lda_dominant_topics.csv',\n",
    "            output_path='lda_output/sunburst_chart.png'\n",
    "        )\n",
    "       \n",
    "        merge_topics(\"lda_output/lda_topic_info.csv\")\n",
    "        plot_topic_distribution_from_db_and_csv(\n",
    "            db_path=config['database_path'], \n",
    "            topics_file='lda_output/tweet_merged_topic_mapping.csv',\n",
    "            topic_info_file='lda_output/lda_dominant_topics.csv',\n",
    "            output_path='lda_output/topic_distribution_by_label_after_merge.png'\n",
    "        )\n",
    "\n",
    "        plot_sunburst_for_top_topics(\n",
    "            db_path=config['database_path'], \n",
    "            topics_file='lda_output/tweet_merged_topic_mapping.csv',\n",
    "            topic_info_file='lda_output/lda_dominant_topics.csv',\n",
    "            output_path='lda_output/sunburst_chart_after_merge.png'\n",
    "        )\n",
    "\n",
    "        log_lda_run(\n",
    "            model=lda_model,\n",
    "            dataset_name=config['database_path'],\n",
    "            optimal_num_topics=optimal_num_topics,\n",
    "            coherence_score=lda_coherence\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Dominant topics DataFrame is empty, skipping distribution plot.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
